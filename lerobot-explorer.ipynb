{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e40af32-6c2d-4c11-bfff-0cf7b37b4fc3",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e557aa0-d2af-4706-ab56-a92dce8cc0f9",
   "metadata": {},
   "source": [
    "## Data Layout Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6ded10-dd25-417f-80dd-6ae7a49897ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_ROOT = Path(\"/Users/nanzhu/code/Isaac-GR00T/demo_data/cube_to_bowl_5\")\n",
    "CHUNK_GLOB = \"chunk-*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "396fc2f0-bc08-4aed-8148-ce431cfa4bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ DATASET ROOT\n",
      "  ‚Ä¢ Path: /Users/nanzhu/code/Isaac-GR00T/demo_data/cube_to_bowl_5\n",
      "\n",
      "üß© CHUNKS (data/video shards)\n",
      "  ‚Ä¢ Found 1 chunks:\n",
      "    - chunk-000\n",
      "\n",
      "üéûÔ∏è EPISODES (from parquet files)\n",
      "  ‚Ä¢ Total episodes discovered: 5\n",
      "  ‚Ä¢ Episode index preview: [0, 1, 2, 3, 4]\n",
      "\n",
      "üìπ VIDEO KEYS (camera / image modalities)\n",
      "  ‚Ä¢ Total video keys (cameras): 2\n",
      "  ‚Ä¢ Video key preview: ['observation.images.front', 'observation.images.wrist']\n",
      "\n",
      "üìÅ META FILES\n",
      "  ‚Ä¢ Meta files found: ['episodes.jsonl', 'info.json', 'modality.json', 'relative_stats.json', 'stats.json', 'tasks.jsonl']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from textwrap import indent\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "@dataclass\n",
    "class DatasetLayout:\n",
    "    root: Path\n",
    "    meta_dir: Path\n",
    "    data_dir: Path\n",
    "    videos_dir: Path\n",
    "    chunks: List[str]\n",
    "\n",
    "    # episodes\n",
    "    parquet_files: List[Path]\n",
    "    episode_indices: List[int]\n",
    "    episode_to_parquet: Dict[int, Path]\n",
    "\n",
    "    # cameras / video keys\n",
    "    video_keys: List[str]\n",
    "    # mapping: video_key -> list of mp4 files\n",
    "    video_files: Dict[str, List[Path]]\n",
    "\n",
    "    # meta files\n",
    "    meta_files: Dict[str, Path]  # filename -> path\n",
    "\n",
    "def _parse_episode_index(p: Path) -> Optional[int]:\n",
    "    # episode_000123.parquet or episode_000123.mp4\n",
    "    m = re.search(r\"episode_(\\d+)\\.(parquet|mp4)$\", p.name)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def discover_layout(root: Path, chunk_glob: str = \"chunk-*\") -> DatasetLayout:\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"DATA_ROOT not found: {root}\")\n",
    "\n",
    "    meta_dir = root / \"meta\"\n",
    "    data_dir = root / \"data\"\n",
    "    videos_dir = root / \"videos\"\n",
    "\n",
    "    # chunks: union from data/chunk-* and videos/chunk-*\n",
    "    data_chunks = sorted([p.name for p in data_dir.glob(chunk_glob) if p.is_dir()]) if data_dir.exists() else []\n",
    "    video_chunks = sorted([p.name for p in videos_dir.glob(chunk_glob) if p.is_dir()]) if videos_dir.exists() else []\n",
    "    chunks = sorted(set(data_chunks + video_chunks))\n",
    "\n",
    "    # parquet files across chunks\n",
    "    parquet_files = []\n",
    "    for ch in chunks:\n",
    "        parquet_files.extend(sorted((data_dir / ch).glob(\"episode_*.parquet\")))\n",
    "    parquet_files = sorted(parquet_files)\n",
    "\n",
    "    episode_indices = []\n",
    "    episode_to_parquet = {}\n",
    "    for pq in parquet_files:\n",
    "        ei = _parse_episode_index(pq)\n",
    "        if ei is None:\n",
    "            continue\n",
    "        episode_indices.append(ei)\n",
    "        episode_to_parquet[ei] = pq\n",
    "    episode_indices = sorted(set(episode_indices))\n",
    "\n",
    "    # discover video keys: videos/chunk-xxx/<video_key>/episode_*.mp4\n",
    "    video_keys = []\n",
    "    video_files: Dict[str, List[Path]] = {}\n",
    "    for ch in chunks:\n",
    "        ch_dir = videos_dir / ch\n",
    "        if not ch_dir.exists():\n",
    "            continue\n",
    "        for vk_dir in sorted([p for p in ch_dir.iterdir() if p.is_dir()]):\n",
    "            vk = vk_dir.name\n",
    "            video_keys.append(vk)\n",
    "            video_files.setdefault(vk, [])\n",
    "            video_files[vk].extend(sorted(vk_dir.glob(\"episode_*.mp4\")))\n",
    "\n",
    "    video_keys = sorted(set(video_keys))\n",
    "    for vk in video_keys:\n",
    "        video_files[vk] = sorted(video_files.get(vk, []))\n",
    "\n",
    "    # meta files\n",
    "    meta_files = {}\n",
    "    if meta_dir.exists():\n",
    "        for p in sorted(meta_dir.iterdir()):\n",
    "            if p.is_file():\n",
    "                meta_files[p.name] = p\n",
    "\n",
    "    return DatasetLayout(\n",
    "        root=root,\n",
    "        meta_dir=meta_dir,\n",
    "        data_dir=data_dir,\n",
    "        videos_dir=videos_dir,\n",
    "        chunks=chunks,\n",
    "        parquet_files=parquet_files,\n",
    "        episode_indices=episode_indices,\n",
    "        episode_to_parquet=episode_to_parquet,\n",
    "        video_keys=video_keys,\n",
    "        video_files=video_files,\n",
    "        meta_files=meta_files,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def pretty_layout_summary(layout: DatasetLayout, preview_n: int = 5):\n",
    "    lines = []\n",
    "\n",
    "    lines.append(\"üì¶ DATASET ROOT\")\n",
    "    lines.append(f\"  ‚Ä¢ Path: {layout.root}\")\n",
    "\n",
    "    lines.append(\"\\nüß© CHUNKS (data/video shards)\")\n",
    "    if layout.chunks:\n",
    "        lines.append(f\"  ‚Ä¢ Found {len(layout.chunks)} chunks:\")\n",
    "        for ch in layout.chunks:\n",
    "            lines.append(f\"    - {ch}\")\n",
    "    else:\n",
    "        lines.append(\"  ‚ö†Ô∏è No chunks found\")\n",
    "\n",
    "    lines.append(\"\\nüéûÔ∏è EPISODES (from parquet files)\")\n",
    "    lines.append(f\"  ‚Ä¢ Total episodes discovered: {len(layout.episode_indices)}\")\n",
    "    if layout.episode_indices:\n",
    "        preview = layout.episode_indices[:preview_n]\n",
    "        lines.append(f\"  ‚Ä¢ Episode index preview: {preview}\"\n",
    "                     + (\"\" if len(layout.episode_indices) <= preview_n else \" ...\"))\n",
    "\n",
    "    lines.append(\"\\nüìπ VIDEO KEYS (camera / image modalities)\")\n",
    "    lines.append(f\"  ‚Ä¢ Total video keys (cameras): {len(layout.video_keys)}\")\n",
    "    if layout.video_keys:\n",
    "        preview = layout.video_keys[:preview_n]\n",
    "        lines.append(f\"  ‚Ä¢ Video key preview: {preview}\"\n",
    "                     + (\"\" if len(layout.video_keys) <= preview_n else \" ...\"))\n",
    "\n",
    "    lines.append(\"\\nüìÅ META FILES\")\n",
    "    if layout.meta_files:\n",
    "        lines.append(f\"  ‚Ä¢ Meta files found: {list(layout.meta_files.keys())}\")\n",
    "    else:\n",
    "        lines.append(\"  ‚ö†Ô∏è No meta files found\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "layout = discover_layout(DATA_ROOT, CHUNK_GLOB)\n",
    "print(pretty_layout_summary(layout))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19285e3-01d6-43bd-bb2a-02783bd021fe",
   "metadata": {},
   "source": [
    "## Feature Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab104290-3e87-4550-bb1e-5cf6411c8f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root': '/Users/nanzhu/code/Isaac-GR00T/demo_data/cube_to_bowl_5',\n",
       " 'chunks': ['chunk-000'],\n",
       " 'num_parquet_files': 5,\n",
       " 'num_episodes_found': 5,\n",
       " 'video_keys': ['observation.images.front', 'observation.images.wrist'],\n",
       " 'num_video_keys': 2,\n",
       " 'meta_files': ['episodes.jsonl',\n",
       "  'info.json',\n",
       "  'modality.json',\n",
       "  'relative_stats.json',\n",
       "  'stats.json',\n",
       "  'tasks.jsonl'],\n",
       " 'codebase_version': 'v2.1',\n",
       " 'robot_type': 'so101_follower',\n",
       " 'total_episodes(meta)': 5,\n",
       " 'total_frames(meta)': 4148,\n",
       " 'fps(meta)': 30,\n",
       " 'total_videos(meta)': 10,\n",
       " 'data_path_template': 'data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet',\n",
       " 'video_path_template': 'videos/chunk-{episode_chunk:03d}/{video_key}/episode_{episode_index:06d}.mp4',\n",
       " 'num_features(meta)': 9,\n",
       " 'feature_keys(meta)': ['action',\n",
       "  'episode_index',\n",
       "  'frame_index',\n",
       "  'index',\n",
       "  'observation.images.front',\n",
       "  'observation.images.wrist',\n",
       "  'observation.state',\n",
       "  'task_index',\n",
       "  'timestamp']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text())\n",
    "\n",
    "info = None\n",
    "info_path = layout.meta_files.get(\"info.json\")\n",
    "if info_path and info_path.exists():\n",
    "    info = read_json(info_path)\n",
    "\n",
    "summary = {\n",
    "    \"root\": str(layout.root),\n",
    "    \"chunks\": layout.chunks,\n",
    "    \"num_parquet_files\": len(layout.parquet_files),\n",
    "    \"num_episodes_found\": len(layout.episode_indices),\n",
    "    \"video_keys\": layout.video_keys,\n",
    "    \"num_video_keys\": len(layout.video_keys),\n",
    "    \"meta_files\": sorted(layout.meta_files.keys()),\n",
    "}\n",
    "\n",
    "# enrich from info.json if present\n",
    "if info is not None:\n",
    "    summary.update({\n",
    "        \"codebase_version\": info.get(\"codebase_version\"),\n",
    "        \"robot_type\": info.get(\"robot_type\"),\n",
    "        \"total_episodes(meta)\": info.get(\"total_episodes\"),\n",
    "        \"total_frames(meta)\": info.get(\"total_frames\"),\n",
    "        \"fps(meta)\": info.get(\"fps\"),\n",
    "        \"total_videos(meta)\": info.get(\"total_videos\"),\n",
    "        \"data_path_template\": info.get(\"data_path\"),\n",
    "        \"video_path_template\": info.get(\"video_path\"),\n",
    "        \"num_features(meta)\": len(info.get(\"features\", {})),\n",
    "        \"feature_keys(meta)\": sorted(list(info.get(\"features\", {}).keys()))[:20],\n",
    "    })\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5470e-3272-4470-943d-aacbe8af63de",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d93634cf-1978-4dbf-b29e-3bbf10eccaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Layout validated.\n",
      "\n",
      "SUMMARY:\n",
      "  ‚Ä¢ root: /Users/nanzhu/code/Isaac-GR00T/demo_data/cube_to_bowl_5\n",
      "  ‚Ä¢ chunks: ['chunk-000']\n",
      "  ‚Ä¢ episodes_found: 5\n",
      "  ‚Ä¢ parquet_files_found: 5\n",
      "  ‚Ä¢ video_keys_found: 2\n",
      "  ‚Ä¢ videos_found_total: 10\n",
      "  ‚Ä¢ meta_files_found: ['episodes.jsonl', 'info.json', 'modality.json', 'relative_stats.json', 'stats.json', 'tasks.jsonl']\n",
      "\n",
      "COVERAGE (per key):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_type</th>\n",
       "      <th>key_name</th>\n",
       "      <th>episodes_present</th>\n",
       "      <th>episodes_missing</th>\n",
       "      <th>coverage_ratio</th>\n",
       "      <th>missing_examples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>parquet</td>\n",
       "      <td>data</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video</td>\n",
       "      <td>observation.images.front</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video</td>\n",
       "      <td>observation.images.wrist</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  key_type                  key_name  episodes_present  episodes_missing  \\\n",
       "0  parquet                      data                 5                 0   \n",
       "1    video  observation.images.front                 5                 0   \n",
       "2    video  observation.images.wrist                 5                 0   \n",
       "\n",
       "   coverage_ratio missing_examples  \n",
       "0             1.0               []  \n",
       "1             1.0               []  \n",
       "2             1.0               []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO.JSON COUNT CHECK:\n",
      "  ‚Ä¢ info.total_episodes: 5\n",
      "  ‚Ä¢ info.total_videos: 10\n",
      "  ‚Ä¢ episodes_found: 5\n",
      "  ‚Ä¢ videos_found_total: 10\n",
      "  ‚Ä¢ match_total_episodes: True\n",
      "  ‚Ä¢ match_total_videos: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def episode_coverage_report(layout: DatasetLayout) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a table:\n",
    "      key_type | key_name | episodes_present | episodes_missing | coverage_ratio\n",
    "    key_type includes: parquet, video\n",
    "    \"\"\"\n",
    "    all_eps = set(layout.episode_indices)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Parquet coverage (baseline)\n",
    "    parquet_eps = set(layout.episode_to_parquet.keys())\n",
    "    rows.append({\n",
    "        \"key_type\": \"parquet\",\n",
    "        \"key_name\": \"data\",\n",
    "        \"episodes_present\": len(parquet_eps),\n",
    "        \"episodes_missing\": len(all_eps - parquet_eps),\n",
    "        \"coverage_ratio\": (len(parquet_eps) / max(1, len(all_eps))),\n",
    "        \"missing_examples\": sorted(list(all_eps - parquet_eps))[:10],\n",
    "    })\n",
    "\n",
    "    # Video coverage per key\n",
    "    for vk in layout.video_keys:\n",
    "        files = layout.video_files.get(vk, [])\n",
    "        have = set()\n",
    "        for f in files:\n",
    "            ei = _parse_episode_index(f)\n",
    "            if ei is not None:\n",
    "                have.add(ei)\n",
    "\n",
    "        miss = sorted(list(all_eps - have))\n",
    "        rows.append({\n",
    "            \"key_type\": \"video\",\n",
    "            \"key_name\": vk,\n",
    "            \"episodes_present\": len(have),\n",
    "            \"episodes_missing\": len(miss),\n",
    "            \"coverage_ratio\": (len(have) / max(1, len(all_eps))),\n",
    "            \"missing_examples\": miss[:10],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values([\"key_type\", \"coverage_ratio\", \"key_name\"], ascending=[True, True, True])\n",
    "    return df\n",
    "\n",
    "def validate_layout(layout: DatasetLayout, info: dict | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a structured validation report:\n",
    "      - summary: high-level counts\n",
    "      - coverage: per-key episode coverage table\n",
    "      - meta_counts_check: compare vs info.json (optional)\n",
    "    Raises on hard errors.\n",
    "    \"\"\"\n",
    "    # Hard errors\n",
    "    if not layout.root.exists():\n",
    "        raise FileNotFoundError(f\"DATA_ROOT not found: {layout.root}\")\n",
    "    if not layout.data_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing data dir: {layout.data_dir}\")\n",
    "    if not layout.videos_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing videos dir: {layout.videos_dir}\")\n",
    "    if len(layout.episode_indices) == 0:\n",
    "        raise RuntimeError(\"No episodes found (no episode_*.parquet under data/).\")\n",
    "\n",
    "    # Coverage table (your main ‚Äúsanity check‚Äù)\n",
    "    coverage = episode_coverage_report(layout)\n",
    "\n",
    "    # High-level summary\n",
    "    summary = {\n",
    "        \"root\": str(layout.root),\n",
    "        \"chunks\": layout.chunks,\n",
    "        \"episodes_found\": len(layout.episode_indices),\n",
    "        \"parquet_files_found\": len(layout.parquet_files),\n",
    "        \"video_keys_found\": len(layout.video_keys),\n",
    "        \"videos_found_total\": int(sum(len(v) for v in layout.video_files.values())),\n",
    "        \"meta_files_found\": sorted(layout.meta_files.keys()),\n",
    "    }\n",
    "\n",
    "    # Optional: compare against info.json\n",
    "    meta_counts_check = None\n",
    "    if info is not None:\n",
    "        meta_counts_check = {\n",
    "            \"info.total_episodes\": info.get(\"total_episodes\"),\n",
    "            \"info.total_videos\": info.get(\"total_videos\"),\n",
    "            \"episodes_found\": len(layout.episode_indices),\n",
    "            \"videos_found_total\": int(sum(len(v) for v in layout.video_files.values())),\n",
    "            \"match_total_episodes\": (info.get(\"total_episodes\") == len(layout.episode_indices)) if isinstance(info.get(\"total_episodes\"), int) else None,\n",
    "            \"match_total_videos\": (info.get(\"total_videos\") == int(sum(len(v) for v in layout.video_files.values()))) if isinstance(info.get(\"total_videos\"), int) else None,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"coverage\": coverage,\n",
    "        \"meta_counts_check\": meta_counts_check,\n",
    "    }\n",
    "\n",
    "report = validate_layout(layout, info)\n",
    "print(\"‚úÖ Layout validated.\\n\")\n",
    "print(\"SUMMARY:\")\n",
    "for k, v in report[\"summary\"].items():\n",
    "    print(f\"  ‚Ä¢ {k}: {v}\")\n",
    "\n",
    "print(\"\\nCOVERAGE (per key):\")\n",
    "display(report[\"coverage\"].sort_values([\"key_type\",\"key_name\"]))\n",
    "\n",
    "if report[\"meta_counts_check\"] is not None:\n",
    "    print(\"\\nINFO.JSON COUNT CHECK:\")\n",
    "    for k, v in report[\"meta_counts_check\"].items():\n",
    "        print(f\"  ‚Ä¢ {k}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lerobot-uv)",
   "language": "python",
   "name": "lerobot-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
